{
	"title":"SNAP Twitter Scrapping and Visualization",
	"scope": "This project includes a twitter–data–scrapping component, with the goal of analyzing Twitter.com posts/tweets made by members of U.S. Senate, and House of Representatives, trying to find patterns in how they frame the policy issues on social media. The code and the application are available publicly on github. We used Python programming language, and Python library ‘TwitterAPI’. The application reads a csv file with twitter handles listed in a column called TwitterHandles, and populates the responds in csv format for each twitter handle/input table rows", 
	"howTo": "", 
	"descri": "In order to retrieve the tweets, we used the Twitter’s REST API, which provides programmatic access to read the Twitter data. We used the twitter handles for all the members of the House of Representatives, as well as the U.S. Senate. Twitter API however limits the number of requests in a time-wise manner, meaning that it responds to 180 requests in each 15 minutes. Also, for any given user, Twitter API returns up to 3200 most recent tweets, however it responds with maximum of 200 tweets per each request. Twitter API receives the request with a set of possible arguments parameters including: user_id, screen_name, since_id, count, max_id, trim_user, exclude_replies, contributor_details, include_rts. Detailed explanations of these parameters, and how setting them defines the responses are available at twitter’s developer documentation.", 
	"challenges": "As explained in the Twitter API documentation, it is not possible to retrieve tweets by the date they were created. This was important to this project because we needed to analyze the tweets in a given time-period of summer of 2013. In order to achieve this, we retrieved each user’s timeline (all tweets) and having the date_created field for each tweet, filtered the results to the needed time–period. Another challenge was the per request limit of number of tweets responded by the Twitter API. In this case, we programed our code to find the greatest tweet–id for each round of respond and pass it as the max_id parameter to the next request it makes. The max_id parameter is used by the API, to respond with the next 200 tweets earlier than the one with the max_id. This recursive process continues until there is no earlier tweets", 
	"images":["api/ts1.png","api/ts2.png"]
}
